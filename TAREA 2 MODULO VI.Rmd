---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Integrantes:**

* Luis Rodrigo Santamaría Rodríguez
* Jonathan Vargas González
* Marisol Estrada Téllez

En esta tarea se abordará el análisis de una base de datos relacionada con la calidad del agua, la cual contiene diversas variables físico-químicas que permiten evaluar su potabilidad. El objetivo principal es aplicar técnicas de agrupamiento (clustering) para identificar patrones ocultos y posibles grupos naturales dentro de los datos, sin necesidad de una variable de salida en este caso nuestra variable llamada "Potability".

La base de datos utilizada incluye atributos como pH, dureza, sólidos disueltos, cloraminas, sulfatos, conductividad eléctrica, carbono orgánico, trihalometanos y turbidez. Cada registro representa una muestra de agua, y aunque se cuenta con la variable Potability como referencia, el enfoque será completamente no supervisado.

Para lograr una segmentación significativa, se emplearán métodos de reducción de dimensionalidad como PCA, t-SNE y UMAP (los cuáles fueron analizados en la tarea 1), que facilitarán la visualización y comprensión de la estructura interna de los datos. Posteriormente, se aplicarán algoritmos de clustering como K-means, DBSCAN, HDBSCAN, etc **(Agregar los que falten)**, con el fin de descubrir agrupamientos que puedan aportar valor en la clasificación de la calidad del agua.


Cargamos las librerias que vamos a utilizar

```{r}
library(tidymodels)
library(tidyverse)
library(tidyclust)
library(factoextra)
library(FactoMineR)
library(cluster)
library(mlr)
library(GGally)
library(ClusterR)
library(vegan)
library(NbClust)
library(gridExtra)
library(grid)
library(lattice)
library(mice)
library(dbscan)
require(igraph)
```

Cargamos nuestras bases de datos.

```{r}
df<-read.csv("./water_potability.csv")

df %>% dim()

df %>% glimpse()

df %>% head()

df %>% summary()
```

Veamos los datos faltantes 
```{r}
colSums(is.na(df))
```
Imputamos datos ya que, tenemos datos faltantes:

```{r}
imputed_data <- mice(df, m = 5, method = 'pmm', maxit = 5, seed = 123)
```
```{r message=FALSE, warning=FALSE}
df <- complete(imputed_data, 1)
```

```{r}
colSums(is.na(df))
```
Convertimos nuestra variable de salida en este caso es "Potability" a Factor ya que, es de tipo int

```{r}
df$Potability <- as.factor(df$Potability)
```

```{r}
table(df$Potability)
```

Hacemos un gráfico de nuestra variable "Potability" 
```{r}
df %>% glimpse()

df %>%
  group_by(Potability) %>%
  summarise (n = n()) %>%
  mutate(prop = n / sum(n)) %>%
ggplot(aes(df,x = Potability, y = n)) +
    geom_col(fill = c("#CC0033", "#e319dc")) +
    geom_text(aes(label = paste0(n, " | ", signif(n / nrow(df) * 100, digits = 4), '%')), nudge_y = 10) + ggtitle("Porcentajes de resultados de biopsia") +
    theme_gray()
```
```{r}
ggpairs(df, mapping = aes(fill = Potability, color = Potability, alpha = 0.5),columns = seq(1,9), lower = "blank", upper = "blank") +
scale_fill_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) + labs(title = "Comparación por gráficos de dispersión") + 
theme(
  axis.text.x = element_text(size = 4),
  axis.text.y = element_text(size = 4)
)
```

```{r}
cor1_data<-cor(df[,-10],method="spearman")

ggcorrplot::ggcorrplot(corr = cor1_data,
                       type = "lower", 
                       show.diag = TRUE,
                       lab = TRUE, 
                       lab_size = 3) +
                       labs(title = "Correlación entre las variables")
```
Vamos aplicar los clusters vistos en clases

## Métodos Jerárquicos

Primero comenzamos con los **Métodos Jerarquicos** 

Para eso vamos a eliminar nuestra variable de salida en este caso la variable "Potability"

```{r}
df1<- df %>% dplyr::select(-Potability)

head(df1)
```

Vamos a calcular la distancia, en este caso como tenemos que nuestra variables son continuas utilizaremos la distancia **Euclidiana**. Cabe mencionar que también vamos a estandarizar los datos para que no tengamos algún sesgo, de las variables que tengan una diferente dimensión a las demás.

```{r}
dist_df <- dist(scale(df1), method = "euclidean")
```

Calculamos nuestras ligas:

```{r}
hc_ave <- hclust(dist_df, method = "average")
hc_single<-hclust(dist_df, method = "single")
hc_comp<-hclust(dist_df, method = "complete")
hc_ward<-hclust(dist_df, method = "ward.D")
hc_ward2<-hclust(dist_df, method = "ward.D2")
hc_centroid <- hclust(dist_df, method = "centroid")
```

Sabemos que las mejores ligas son las Ward.

Calculamos la distancia cophenetic y calculamos la correlación que tienen las ditancias. 

```{r}
d_ave <- cophenetic(hc_ave)
d_single <- cophenetic(hc_single)
d_comp<- cophenetic(hc_comp)
d_ward<- cophenetic(hc_ward)
d_ward2<-cophenetic(hc_ward2)
d_centroid<-cophenetic(hc_centroid)

cbind(cor(dist_df,d_ave),cor(dist_df,d_single),cor(dist_df,d_comp),cor(dist_df,d_ward),cor(dist_df,d_ward2), cor(dist_df, d_centroid))
```

Podemos obeservar que las mejores ligas que mantienen la distancia son la promedio y la simple

Vamos a graficarlas

```{r}
plot(hc_ave)
```
```{r}
plot(hc_single)
```

```{r}
plot(hc_centroid)
```
Podemos observar que con los dendogramas con las ligas single y average no podemos afirmar cuántos grupos podemos tener de toda nuestra base de datos haciendo algún corte.

Unas ligas que generalmente funcionan "mejor" son las de ward

```{r}
plot(hc_ward)
```

Podemos notar en la gráfica anterior que si hacemos un corte a la altura 215 aproximadamente tenemos un total de 4 clusters y si hacemos un corte a una altura de 250 tenemos 3 clusters.

A continuación veamos la grafica de Ward 2

```{r}
plot(hc_ward2)
```

Podemos observar del dendograma anterior que si hacemos un corte con altura 45 tenemos 2 clusters, pero si hacemos un corte con una altura de 40 tenemos 4 clusters.

Con los dendogramas anteriores no podemos hacer una afirmación de cuantos cluster podemos tener en nuestra base de datos.

```{r}

fviz_dend(hc_ward, cex = 0.65, k = 3, color_labels_by_k = FALSE, rect = TRUE, rect_fill = TRUE, k_colors = "jco", rect_border = "jco", show_labels = F, main="Cluster: Liga ward dendrograma (3 grupos)")

fviz_dend(hc_ward2, cex = 0.65, k = 3, color_labels_by_k = FALSE, rect = TRUE, rect_fill = TRUE, k_colors = "jco", rect_border = "jco", show_labels = F, main="Cluster: Liga ward2 dendrograma (3 grupos)")

fviz_dend(hc_ward, cex = 0.8, k = 3,k_colors =c("#FC4E07","#a8a632", "#00AFBB") ,type = "rectangle")

fviz_dend(hc_ward, cex = 0.8, k = 3,k_colors =c("#FC4E07","#a8a632", "#00AFBB") ,type = "circular")

fviz_dend(hc_ward, cex = 0.8, k = 3,k_colors =c("#FC4E07","#a8a632", "#00AFBB") ,type = "phylogenic")

fviz_dend(hc_ward, cex = 0.8, k = 3,k_colors =c("#FC4E07","#a8a632", "#00AFBB") ,type = "phylogenic", repel=TRUE)

```

Hasta el momento aún no podemos afirmar que tengamos algún número de clusters fijo.

### AGNES

Trabajemos con otro método aglomerativo llamado agnes, pero también vamos a estandarizar nuestros datos.

```{r}
df1<-as.data.frame(scale(df1))

ac_metric <- list(
  complete_ac = agnes(df1, metric = "euclidean", method = "complete")$ac,
  average_ac = agnes(df1, metric = "euclidean", method = "average")$ac,
  single_ac = agnes(df1, metric = "euclidean", method = "single")$ac,
  ward_ac = agnes(df1, metric = "euclidean", method = "ward")$ac
)

ac_metric
```

```{r}
df1 |> head()
summary(df1) # Verificar que nuestros datos esten escalados
```

Podemos observar que la mejor liga con el método agnes es la Ward

Veamos su gráfica

```{r}
plot(agnes(df1, metric = "euclidean", method = "ward"))
```

Podemos observar que tenemos un comportamiento similar a las dos ligas Ward anteriores. Por lo que podemos decir que si hacemos un corte con altura alrededor de 40 tenemos tres clusters.

### DIANA

Veamos con otro método divisivo llamado DIANA

```{r}
plot(diana(df1, metric = "euclidean"))
```

Podemos observar que con DIANA tenemos 2 clusters si hacemos un corte alrededor de altura 11

Con los métodos jerarquicos podemos afirmar que tenemos entre 2 y 3 clusters

Ahora hagamos el análisis pero con tidymodels usando la liga promedio

```{r}
hc_spec <- hier_clust(
  linkage_method = "average" )%>%  
  set_engine("stats")

hc_spec

hclust_fit <- tidyclust::fit(hc_spec, ~., df1)

hclust_fit 

hclust_fit$fit %>% plot()

```
Observemos que no es claro

Hagamos el analísis pero sin declarar alguna liga

```{r}
hclust_spec <- hier_clust() %>%
               set_engine("stats")

hclust_fit1 <- tidyclust::fit(hclust_spec, ~., df1)

hclust_fit1 %>%
  extract_centroids(num_clusters = 3)%>%print(Inf)

hclust_fit1$fit %>% plot()

```
Observemos que tampoco es tan claro como las ligas Ward, en este caso la gráfica anterior utiliza la liga completa

Veamos las gráficas pero ahora declarando las ligas que usamos anteriormente

**Liga Completa**
```{r}
res_hclust_complete <- hier_clust(linkage_method = "complete") %>%
  tidyclust::fit(~., data = df1)
```


```{r}
res_hclust_complete$fit %>% plot()
```


**Liga Promedio**

```{r}
res_hclust_average <- hier_clust(linkage_method = "average") %>%
  tidyclust::fit(~., data = df1)

res_hclust_average$fit %>% plot()
```

**Liga Simple**

```{r}
res_hclust_single <- hier_clust(linkage_method = "single") %>%
  tidyclust::fit(~., data = df1)

res_hclust_single$fit %>% plot()
```

**Liga Ward D**

```{r}
res_hclust_ward <- hier_clust(linkage_method = "ward.D") %>%
  tidyclust::fit(~., data = df1)

res_hclust_ward$fit %>% plot()
```

**Liga Ward D2**


```{r}
res_hclust_ward2 <- hier_clust(linkage_method = "ward.D2") %>%
  tidyclust::fit(~., data = df1)

res_hclust_ward2$fit %>% plot()

```

Podemos observar que usando tidymodels con las ligas Ward son las que nos dan más claridad de cuántos clusters podemos tener en nuestros datos en este caso tenemos de 2 a 3 dependiendo a que altura cortemos.

## Métodos no Jerárquicos

### K-MEANS

Primero veremos el estadístico GAP para ver el número de clusters que puede ser adecuado.

```{r}
fviz_nbclust(df1, kmeans, method = "gap_stat")+labs(x ="Número de clusters")+labs(y="GAP")+labs(title = "Número óptimo de clusters")
```

Con K-Means indica que el número óptimo de clusters es 1, es decir que no encuentra más de un cluster como óptimo.

(Aún así) Para el método de K-Means probaremos con valores de K de 2, 3 y 4, y mediante validación cruzada escogeremos el valor de K óptimo.

```{r}
dfTask <- makeClusterTask(data = df1)
listLearners("cluster")$class

kMeans <- makeLearner("cluster.kmeans",par.vals = list(iter.max = 2500, nstart = 25))

kMeansParamSpace <- makeParamSet(
makeDiscreteParam("centers", values = c(2:4)),
makeDiscreteParam("algorithm",
values = c("Hartigan-Wong", "Lloyd", "MacQueen")))
gridSearch <- makeTuneControlGrid()
kFold <- makeResampleDesc("CV", iters = 25)

set.seed(123)

tunedK <- tuneParams(kMeans, task = dfTask,
resampling = kFold,
par.set = kMeansParamSpace,
control = gridSearch,
measures = list(db, G1))
```

```{r}
set.seed(123)
kMeansTuningData <- generateHyperParsEffectData(tunedK)
kMeansTuningData$data
gatheredTuningData <- gather(kMeansTuningData$data,
key = "Metric",
value = "Value",
c(-centers, -iteration, -algorithm))

ggplot(gatheredTuningData, aes(centers, Value, col = algorithm)) +
facet_wrap(~ Metric, scales = "free_y") +
geom_line() +
geom_point() +
theme_bw()
```

En la gráfica anterior vemos que el mejor modelo es con K = 4.

```{r}
set.seed(123)
tunedKMeans <- setHyperPars(kMeans, par.vals = tunedK$x)
tunedKMeansModel <- train(tunedKMeans, dfTask)
kMeansModelData <- getLearnerModel(tunedKMeansModel)
kMeansModelData$iter

tunedKMeans

df_clus <- df %>% mutate(kMeansCluster = as.factor(kMeansModelData$cluster))
table(df_clus$kMeansCluster)

ggpairs(df_clus, aes(col = kMeansCluster),upper = list(continuous = "density")) +
theme_bw()
```

En los gráficos anteriores podemos ver que no se percibe una buena separación de los clusters. Además, tampoco se ve que los clusters estén relacionados con la potabilidad del agua, pues están casi igualmente distribuidos en ambos grupos de potabilidad.


### DBSCAN

Para el parámetro del número de vecinos cercanos tomaremos 10 por tener 9 variables del modelo.
```{r}
kNNdistplot(df1, k =10)
abline(h=c(2, 3),lty=2,col="blue")
```

En la gráfica anterior podemos ver que un intervalo donde podemos ver el punto de inflexión en las distancias es entre 2 y 3.

Si calculamos los cuantiles, vemos que el 95% de las distancias quedamos debajo de 2.71. Por lo que un valor adecuado de epsilón podría ser 3.

```{r}
quantile(kNNdist(df1, k = 10), c(0.70,0.75, 0.8, 0.9, 0.95, 0.99, 0.995, 0.999))
```

```{r}
df_clus <- dbscan(df, eps = 3, minPts = 10)
df_clus
```

Los resultados indican 10 observaciones de ruido y el resto pertenece a un solo grupo.

### HDBSCAN

```{r}
hdb <- hdbscan(df1, minPts = 10) #con minPts = 20 también solo encuentra un grupo
unique(hdb$cluster)

# datos_hdbscan <- data.frame(df, cluster = as.factor(hdb$cluster))
```

Usando HDBSCAN tampoco encuentra más de un solo grupo.
